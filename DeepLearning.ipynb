{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : Handwriting Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the function *f* is represented as neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of neural network\n",
    "-  Neuron: a function that takes input and produces output\n",
    "-  Layer: a set of neurons\n",
    "-  Network: a set of layers\n",
    "- Bias: a parameter that is added to the output of a neuron\n",
    "- Weight: a parameter that is multiplied to the output of a neuron\n",
    "- activation function: a function that is applied to the output of a neuron\n",
    "- Input layer: the first layer of a network\n",
    "- Output layer: the last layer of a network\n",
    "- Hidden layer (Deep layers): a layer between the input and output layers\n",
    "- Cost function: a function that measures the difference between the output of a network and the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between weight and bias\n",
    "- Bias is added to the output of a neuron\n",
    "- Weight is multiplied to the output of a neuron\n",
    "\n",
    "output = activation_function(weight * input + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "-  Sigmoid function: $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "-  Tanh function: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "-  ReLU function: $f(x) = max(0, x)$\n",
    "-  Softmax function: $f(x) = \\frac{e^x}{\\sum_{i=1}^n e^x}$\n",
    "-  Linear function: $f(x) = x$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "-  Dense layer: a layer that is fully connected to the previous layer\n",
    "-  Convolutional layer: a layer that is connected to a subset of the previous layer\n",
    "-  Pooling layer: a layer that is connected to a subset of the previous layer\n",
    "- Softmax layer: a layer that is connected to the previous layer\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation\n",
    "-  Forward propagation: the process of calculating the output of a neural network\n",
    "-  Backward propagation: the process of calculating the gradient of the loss function with respect to the parameters of a neural network\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions\n",
    " Cost can be Euclidean distance, cross entropy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to minimize the cost function. (Backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plateau: a region where the gradient is small but the cost function is not minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saddle point: a point where the gradient is zero but the cost function is not minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using momentum to avoid plateau and saddle point. Note that momentum is not a method to avoid plateau and saddle point, but a method to accelerate the convergence of gradient descent. Not guaranteed to reach the global minimum, but gives hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "-  Gradient descent: a method to minimize the cost function\n",
    "-  Momentum: a method to accelerate the gradient descent\n",
    "-  RMSprop: a method to accelerate the gradient descent\n",
    "-  Adam: a method to accelerate the gradient descent\n",
    "-  Learning rate: a parameter that controls the step size of the gradient descent\n",
    "-  Batch size: a parameter that controls the number of samples used to calculate the gradient\n",
    "-  Epoch: a parameter that controls the number of iterations of the gradient descent\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Batch gradient descent: use all the samples to calculate the gradient\n",
    "-  Stochastic gradient descent: use one sample to calculate the gradient\n",
    "-  Mini-batch gradient descent: use a subset of the samples to calculate the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch is faster and better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deeper better\n",
    "\n",
    "Any continuos function f,\n",
    "$ f: R^n \\rightarrow R^m $,\n",
    "can be represented by a neural network with one hidden layer with n neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fat Layer: a layer with many neurons\n",
    "- Thin Layer: a layer with few neurons\n",
    "- Short Layer: a layer with few layers\n",
    "- Tall Layer: a layer with many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shallower network: a network with few layers\n",
    "- Deeper network: a network with many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer + Learnable kernal + Simple Classifer = Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "ReLu is better than sigmoid because it is faster to compute and it does not saturate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradient problem: the gradient of the sigmoid function is small, which makes it difficult to train the network.\n",
    "\n",
    "In ReLU, vanish gradient problem does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros\n",
    "- Fast to compute\n",
    "- No Vanishing gradient problem\n",
    "- Thinner network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxout\n",
    "$f(x) = max(w_1^Tx + b_1, w_2^Tx + b_2)$\n",
    "\n",
    "ReLU is a special case of Maxout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learnable activation function: a function that is learned by the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "- Learning rate: a parameter that controls the step size of the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if LR is large, cost may not decrease after each update.\n",
    "\n",
    "if LR is small, Training would be slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Adagrad is an adaptive learning rate method.\n",
    "\n",
    "- Adaptive learning rate: a method that changes the learning rate during the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derivative: the gradient of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller derivative means larger learning rate. Larger derivative means smaller learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "\n",
    "- Adam: a method to accelerate the gradient descent\n",
    "- RMSprop: a method to accelerate the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "- Dropout: a method to prevent overfitting.\n",
    "\n",
    " Dropout is only in training, not in testing.\n",
    "\n",
    " Dropout is only in hidden layers, not in input and output layers.\n",
    "\n",
    " It is a kind of ensemble method.\n",
    "\n",
    " - Ensemble method: a method that combines multiple models to make a prediction\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "- Convolutional layer: a layer that is connected to a subset of the previous layer\n",
    "- Pooling layer: a layer that is connected to a subset of the previous layer\n",
    "- Convolutional neural network: a neural network that contains convolutional layers and pooling layers\n",
    "\n",
    "Multiple convolutional layers and pooling layers are stacked together to form a convolutional neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between convolutional layer and fully connected layer\n",
    "- Convolutional layer is connected to a subset of the previous layer\n",
    "- Fully connected layer is fully connected to the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each convolutional layer has a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter: a matrix that is used to calculate the output of a convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature map: the output of a convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stride: the step size of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layer does not have filter. It is calculated by weight and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convolutional neural network, weights are shared across the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input image -> Convolutional layer -> Max pooling layer -> Convolutional layer -> Max pooling layer -> Flatten -> Fully connected Feedforward Network -> Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max pooling: a method to reduce the size of the feature map\n",
    "- Flatten: a method to flatten the feature map to a vector\n",
    "- Feedforward: the process of calculating the output of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Max pooling is used to reduce the complexity of the network.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN in Keras\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(25, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(50, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/cnn1.png\">\n",
    "<img src=\"assets/cnn2.png\">\n",
    "<img src=\"assets/cnn3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Plot 2 graphs: training loss and validation loss.\n",
    "\n",
    "loss vs epoch\n",
    "\n",
    "Early stopping is done when the validation loss does not decrease for a certain number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point, validation loss will start to increase. This is the point of overfitting. We should stop training at this point. This is called early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/earlystop.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
