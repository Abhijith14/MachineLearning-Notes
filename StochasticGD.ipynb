{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the gradient to minimise loss function. But it is computationally expensive to compute the gradient for each data point. So we use stochastic gradient descent to minimise the loss function. \n",
    "\n",
    "In stochastic gradient descent we use a subset of data to compute the gradient. The batch size is a hyperparameter. The batch size is the number of data points we use to compute the gradient. The batch size is usually a power of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 epoch is one pass through the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "teta t + 1 = teta t - learning rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of using one sample, we use a batch of samples to compute the gradient. The batch size is a hyperparameter. The batch size is the number of data points we use to compute the gradient. The batch size is usually a power of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is hyperparameter?\n",
    "\n",
    "it is a parameter that is not learned by the model. It is set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD vs Mini-batch gradient descent:\n",
    "- Mini-batch gradient has more cost at each step than SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:\n",
    "- batch size : number of data points to use to compute the gradient\n",
    "- learning rate : how fast we move towards the minimum\n",
    "- number of epochs : number of times we go through the entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergance rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is convergence rate?\n",
    "\n",
    "it is the rate at which the loss function converges to the minimum.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is computatuional cost?\n",
    "\n",
    "it is the number of operations we need to perform to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational cost of logistic regression: O(nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
