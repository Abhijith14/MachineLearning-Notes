{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM: Support Vector Machine is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Splitting the hyperplane with maximum margin.\n",
    "2. Create a SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick : The kernel trick is a method of using a linear classifier to find a non-linear decision boundary. It is a technique to transform data and then find the best possible hyperplane to separate the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a linear classifier (like linear regression). It means that it can only separate data using a straight line. But what if the data is not linearly separable? The kernel trick is a method of using a linear classifier to find a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x, w) = \\sum_{i=1}^{n} w_i x_i  + w_0$$\n",
    "\n",
    "where w is the weight vector, x is the input vector, w0 is the bias and n is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "The decision boundary is the line that separates the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be as far away from the nearest data points as possible. Or margin should be maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margin : The distance between the nearest data point and the decision boundary is called margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the distance to origin:\n",
    "    $$ b \\frac{||w||}{w} = z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where w is the weight vector, b is the bias and z is the distance to origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation for the margin:\n",
    "    $$ \\frac{||w||}{w} = \\frac{2}{||w||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scaling of the data is important for SVM. If the data is not scaled, the SVM will not work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is scaling : Scaling is a method of transforming the data into a common range of values. It is also known as normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max margin optimization problem : The goal of the SVM is to find the hyperplane that maximizes the margin. The margin is the distance between the hyperplane and the nearest training data point of either class. The hyperplane for which this distance is the greatest is the one we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization solution is given by:\n",
    "\n",
    "$$ w = \\sum_{i=1}^{n} \\alpha_i y_i x_i ; w_0 = y_k - w^T x_k$$\n",
    "\n",
    "for any k such that $\\alpha_k \\neq 0$\n",
    "\n",
    "where w is the weight vector, w0 is the bias, x is the input vector, y is the output vector, n is the number of features and k is the index of the support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard margin SVM : In hard margin SVM, we try to fit the largest possible street between the two classes. If the data is not linearly separable, this will not work. The width of the street is called the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft margin SVM : In soft margin SVM, we allow some of the data points to be on the wrong side of the street. These are called the support vectors. The width of the street is called the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft margin classification solution:\n",
    "\n",
    "$$ w = \\sum_{i=1}^{n} \\alpha_i y_i x_i ; w_0 = y_k(1- \\epsilon_k) - \\sum_{i=1}^{n} \\alpha_i y_i x_i^T x_k$$\n",
    "\n",
    "for any k such that $\\alpha_k \\neq 0$\n",
    "\n",
    "where w is the weight vector, w0 is the bias, x is the input vector, y is the output vector, n is the number of features and k is the index of the support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for test point;\n",
    "\n",
    "$$ f(x_*) = \\alpha_i y_i x_i^T x_* + w_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "\n",
    "The kernel trick is a method of using a linear classifier to find a non-linear decision boundary. It is a technique to transform data and then find the best possible hyperplane to separate the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if every data point is mapped to a higher dimension via some transformation;\n",
    "\n",
    "$ \\phi: \\phi(x)$; then\n",
    "\n",
    "$$k(x, x') = \\phi(x)^T \\phi(x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
