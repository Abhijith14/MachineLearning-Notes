{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Modelling\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Neural Network\n",
    "Fixed sized input -> fixed size output\n",
    "\n",
    "Eg: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One to Many\n",
    "Single input -> Sequence output\n",
    "\n",
    "Eg: Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many to One\n",
    "Sequence input -> Single output\n",
    "\n",
    "Eg: Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many to Many\n",
    "Sequence input -> Sequence output\n",
    "\n",
    "Eg: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synced Seuqence input -> Sequence output\n",
    "\n",
    "Eg: Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is updated as a sequence is processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x<sub>t</sub> is the input at time t\n",
    "- h<sub>t</sub> is the hidden state at time t\n",
    "- y<sub>t</sub> is the output at time t\n",
    "- A represents the recurrent cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Hidden State:\n",
    "$$h_t = f_w(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f_w is the function that updates the hidden state with parameters w. (eg: tanh)\n",
    "- h<sub>t-1</sub> is the previous hidden state\n",
    "- x<sub>t</sub> is the current input\n",
    "- h<sub>t</sub> is the new hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNN is a special case of RNN where the recurrent cell is a single layer neural network.\n",
    "\n",
    "$h_t = f_w(h_{t-1}, x_t)$\n",
    "\n",
    "$f_w = tanh(X)$\n",
    "\n",
    "$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph Across Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "represent as computational graph across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradients of a neural network.\n",
    "\n",
    "Gradient Descent:\n",
    "- Initialize weights randomly\n",
    "- Loop until convergence:\n",
    "    - Compute gradients, $\\frac{\\partial L(w)}{\\partial w}$\n",
    "    - Update weights, $w = w - \\alpha \\frac{\\partial L(w)}{\\partial w}$\n",
    "- Return weights\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "$$\\frac{\\partial L(w)}{\\partial w} = \\frac{\\partial L(w)}{\\partial y} \\frac{\\partial y}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "- Chain rule across time\n",
    "$$\\frac{\\partial F(\\theta)}{\\partial h_t} = \\sum_{t'=t}^T \\frac{\\partial F(\\theta)}{\\partial h_{t'}} \\frac{\\partial h_{t'}}{\\partial h_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Flow of Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients by BPTT\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems of BPTT\n",
    "\n",
    "- Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of Long Term Dependencies\n",
    "\n",
    "- Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: LSTM\n",
    "\n",
    "- Long Short Term Memory (LSTM): a special type of RNN that can learn long-term dependencies.\n",
    "- Gated Recurrent Unit (GRU): a gating mechanism that is simpler than LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
